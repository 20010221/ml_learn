{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "简答题：\n",
    "1. 如果你的训练集具有数百万个特征，那么可以使用哪种线性回归训练算法？\n",
    "\n",
    "2. 如果你的训练集里特征的数值大小迥异，那么哪些算法可能会受到影响？受影响程度如何？你应该怎么做？\n",
    "\n",
    "3. 训练逻辑回归模型时，梯度下降可能会卡在局部最小值中吗？\n",
    "\n",
    "4. 如果你让它们运行足够长的时间，是否所有的梯度下降算法都能得出相同的模型？\n",
    "\n",
    "5. 假设你使用批量梯度下降，并在每个轮次绘制验证误差。如果你发现验证错误持续上升，那么可能是什么情况？你该如何解决？\n",
    "\n",
    "6. 当验证误差上升时立即停止小批量梯度下降是个好主意吗？\n",
    "\n",
    "7. 哪种梯度下降算法（在我们讨论过的算法中）将最快地到达最佳解附近？哪个实际上会收敛？如何使其他的也收敛\n",
    "\n",
    "8. 假设你正在使用多项式回归。绘制学习曲线后，你会发现训练误差和验证误差之间存在很大的差距。发生了什么？解决此问题的三种方法是什么？\n",
    "\n",
    "9. 假设你正在使用岭回归，并且你注意到训练误差和验证误差几乎相等且相当高。你是否会说模型存在高偏差或高方差？你应该增加正则化超参数α还是减小它呢？\n",
    "\n",
    "10. 为什么要使用：a.岭回归而不是简单的线性回归（即没有任何正则化）？b.Lasso而不是岭回归？c.弹性网络而不是Lasso回归？\n",
    "\n",
    "11. 假设你要将图片分类为室外/室内和白天/夜间。你应该实现两个逻辑回归分类器还是一个softmax回归分类器？"
   ],
   "id": "6af774aa03256f6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. 随机梯度下降 小批量梯度下降\n",
    "2. 梯度下降,SVM ; 导致损失函数扁平，梯度下降很慢，svm会被数值特征主导；对特征进行标准化，或者缩放\n",
    "3. 不会\n",
    "4. 不会\n",
    "5. 可能是模型过拟合； 可以增加轮次，增加数据\n",
    "6. 不是\n",
    "7. 随机梯度下降 ； 批量梯度下降 ； 对其他的采用学习率衰减策略\n",
    "8. 模型过拟合；  1 正则化  2 降低高项式 3 增加训练数据\n",
    "9.  会 ； 减小它\n",
    "10. a 岭回归加入L2正则化，可以缓解过拟合；  b  lasso采用L1正则化 会使部分特征系数收缩至0 ； c 弹性网络结合 L1 和 L2 正则化，既能进行特征选择，又能保留相关特征\n",
    "11. 两个逻辑回归分类器。\n"
   ],
   "id": "62aed71ab0b305f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "编程题：",
   "id": "10695eaf8992f30e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-04T14:26:42.529550Z",
     "start_time": "2025-08-04T14:26:42.513938Z"
    }
   },
   "source": [
    "# todo 编程题: 在不使用sklearn的情况下，仅使用Numpy，为softmax回归实现带早停的批量梯度下降，将它用于分类任务，\n",
    "#  例如鸢尾花数据集  load_iris, 只用两个特征就可以：\"petal width (cm)\", \"petal length (cm)\"\n",
    "#  强调：除了读数据，其他全用numpy （包括分离测试+验证），不用sklearn\n",
    "\n",
    "#  注意：\n",
    "#  1. 要实现l2正则化\n",
    "#  2. 除了数据读取，其他仅使用numpy，包括训练集+验证集分离，以及softmax预测 和 损失计算"
   ],
   "outputs": [],
   "execution_count": 183
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:47:56.220484Z",
     "start_time": "2025-08-07T14:47:56.212965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "iris = load_iris(as_frame=True)\n",
    "X_iris = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y_iris = iris.target"
   ],
   "id": "e570eec2e1cc8291",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:46:41.392974Z",
     "start_time": "2025-08-07T13:46:41.388722Z"
    }
   },
   "cell_type": "code",
   "source": "X_iris.shape,y_iris.shape",
   "id": "7f645e70d9bab38c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 2), (150,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:58:09.388325Z",
     "start_time": "2025-08-07T14:58:09.384009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_mean = np.mean(X_iris, axis=0)\n",
    "X_std = np.std(X_iris, axis=0)\n",
    "X_iris = (X_iris - X_mean) / X_std"
   ],
   "id": "a85ee8e55387d019",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:58:10.455572Z",
     "start_time": "2025-08-07T14:58:10.451942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, y_train = X_iris[:120],y_iris[:120]\n",
    "X_iris_test,y_iris_test = X_iris[120:],y_iris[120:]"
   ],
   "id": "e39861519aeaf23b",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T15:01:10.743759Z",
     "start_time": "2025-08-07T15:01:10.736253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SoftmaxRegressionWithL2:\n",
    "    def __init__(self, learning_rate=0.01, lambda_reg=0.1, epochs=100000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.epochs = epochs\n",
    "        self.theta = None\n",
    "        self.biases = None\n",
    "\n",
    "\n",
    "    def softmax(self,x):\n",
    "        exp_x = np.exp(x - np.max(x,axis=1,keepdims=True))\n",
    "        return exp_x / np.sum(exp_x,axis=1,keepdims=True)\n",
    "\n",
    "    def compute_loss(self,y_true, y_pred):\n",
    "        m = len(y_true)\n",
    "        cross_entropy = -np.log(y_pred[np.arange(m), np.argmax(y_true, 1)])\n",
    "        l2_loss = (self.lambda_reg / (2 * m)) * np.sum(self.theta ** 2)\n",
    "        return np.mean(cross_entropy) + l2_loss\n",
    "    def to_onehot(self,y,num_classes):\n",
    "        m = y.shape[0]\n",
    "        onehot = np.zeros((m, num_classes))\n",
    "        onehot[np.arange(m), y] = 1\n",
    "        return onehot\n",
    "    def fit(self,X, y):\n",
    "        m, n = X.shape\n",
    "        num_classes = len(np.unique(y))\n",
    "        y_onehot = self.to_onehot(y, num_classes)\n",
    "        K = num_classes\n",
    "        self.theta = np.random.randn(n, K) * 0.01\n",
    "        self.bias = np.zeros((1, K))\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            z = X.dot(self.theta) + self.bias\n",
    "            y_pred = self.softmax(z)\n",
    "\n",
    "            loss = self.compute_loss(y_onehot, y_pred)\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"epoch: {epoch}, loss: {loss:.4f}\")\n",
    "            error = y_pred - y_onehot\n",
    "            theta_a = X.T.dot(error) /m + (self.lambda_reg / m) * self.theta\n",
    "            bias_a = np.mean(error,axis=0,keepdims=True)\n",
    "\n",
    "            self.theta -= self.learning_rate * theta_a\n",
    "            self.bias -= self.learning_rate * bias_a\n",
    "    def predict(self,X):\n",
    "        z = X.dot(self.theta) + self.bias\n",
    "        y_pred = self.softmax(z)\n",
    "        return np.argmax(y_pred,axis=1)\n"
   ],
   "id": "760c2dde19da652c",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T15:01:12.334780Z",
     "start_time": "2025-08-07T15:01:11.717622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SoftmaxRegressionWithL2(learning_rate=0.01, lambda_reg=0.01, epochs=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_iris_test)\n",
    "y_true = y_iris_test\n",
    "accuracy = np.mean(y_pred == y_true)\n",
    "print(f\"测试集准确率: {accuracy:.4f}\")"
   ],
   "id": "97ccf5dbc4721d1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.0904\n",
      "epoch: 1000, loss: 0.3466\n",
      "epoch: 2000, loss: 0.2523\n",
      "epoch: 3000, loss: 0.2059\n",
      "epoch: 4000, loss: 0.1780\n",
      "epoch: 5000, loss: 0.1593\n",
      "epoch: 6000, loss: 0.1457\n",
      "epoch: 7000, loss: 0.1355\n",
      "epoch: 8000, loss: 0.1275\n",
      "epoch: 9000, loss: 0.1211\n",
      "测试集准确率: 0.8667\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b55bfa13b92f3ebb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
